{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics,preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "# Imports for PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#constant declaration\n",
    "batch_size = 30\n",
    "num_classes  = 2\n",
    "epochs = 3\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For full dataset, run tran_feature_selection notebook before this one\n",
    "# For small subset of data, run tran_sample_preprocessing notebook beofore this one (ideal for testing model on CPU)\n",
    "\n",
    "%store -r benign_flows  \n",
    "%store -r mixed_flows\n",
    "%store -r features\n",
    "\n",
    "dim = len(features)\n",
    "\n",
    "\n",
    "normalizer = preprocessing.Normalizer(norm=\"l2\")\n",
    "normalized_train = normalizer.fit_transform(benign_flows[features]) #axis?\n",
    "train_X = pd.DataFrame(normalized_train, columns = features)  #dataframe\n",
    "\n",
    "normalized_test = normalizer.transform(mixed_flows[features])\n",
    "test_X = pd.DataFrame(normalized_test, columns = features)\n",
    "test_y = mixed_flows.is_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/grad/ifath/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = benign_flows.loc[:, features].values\n",
    "x = StandardScaler().fit_transform(x) # normalizing the features\n",
    "np.mean(x),np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the top k components whose cumulative variance exceeds 95%\n",
    "  \n",
    "pca = PCA(0.95)\n",
    "pca.fit(x)\n",
    "top_k_component = pca.n_components_\n",
    "top_k_component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-run PCA with only top k features again\n",
    "top_k_features = features[:top_k_component]\n",
    "\n",
    "top_k_x = benign_flows.loc[:, top_k_features].values\n",
    "top_k_x = StandardScaler().fit_transform(top_k_x) # normalizing the features\n",
    "np.mean(top_k_x),np.std(top_k_x)\n",
    "\n",
    "top_k_pca = PCA(n_components=top_k_component-1)   \n",
    "top_k_principalComponents = top_k_pca.fit_transform(top_k_x)\n",
    "\n",
    "top_k_var=np.cumsum(np.round(top_k_pca.explained_variance_ratio_, decimals=3)*100)\n",
    "# print(top_k_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse mapping of low dimensional to original coordinate space\n",
    "data_reduced = np.dot(x-pca.mean_, pca.components_.T) # transform\n",
    "data_original = np.dot(data_reduced, pca.components_)+ pca.mean_ # inverse_transform\n",
    "re_error = data_original-x  #reconstruction error\n",
    "print(re_error.shape)\n",
    "print(re_error.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['is_attack']\n",
    "\n",
    "x_train = x\n",
    "\n",
    "benign_flows['is_attack']= 0\n",
    "y_train = benign_flows.loc[:, labels].values\n",
    "\n",
    "x_test = mixed_flows.loc[:, features].values\n",
    "y_test = mixed_flows.loc[:, labels].values    \n",
    "\n",
    "# normalizing\n",
    "#x_train = preprocessing.normalize(x_train, axis=0)\n",
    "x_test = preprocessing.normalize(x_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction loss for train partition (benign flow data)\n",
    "from numpy import linalg as LA\n",
    "\n",
    "start=2\n",
    "error_record=[]\n",
    "for i in range(start,top_k_component):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca2_results = pca.fit_transform(x_train)\n",
    "    pca2_proj_back=pca.inverse_transform(pca2_results)\n",
    "    total_loss=LA.norm((x_train-pca2_proj_back),None)\n",
    "    error_record.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "# plt.figure(figsize=(15,15))\n",
    "# plt.title(\"reconstruction error of pca\")\n",
    "# plt.xlabel(\"components\")\n",
    "# plt.ylabel(\"loss value \")\n",
    "# plt.plot(error_record,'r')\n",
    "# plt.xticks(range(len(error_record)), range(start,top_k_component), rotation='vertical')\n",
    "# plt.xlim([-1, len(error_record)])\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction loss for test partition (mixed flow data)\n",
    "\n",
    "start=2\n",
    "test_loss_pca=[]\n",
    "\n",
    "for i in range(start,dim):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca2_results = pca.fit_transform(x_test)\n",
    "    pca2_proj_back=pca.inverse_transform(pca2_results)\n",
    "    total_loss=LA.norm((x_test-pca2_proj_back),None)\n",
    "    test_loss_pca.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.95)\n",
    "train_pca = pca.fit_transform(x_train)\n",
    "test_pca = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary libraries to run the deep learning model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(top_k_component,)))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling and training the model with the reduced dimension\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_pca, y_train,batch_size=batch_size,epochs=epochs,verbose=1,\n",
    "                    validation_data=(test_pca, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how the model works with all the features\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(dim,)))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(train_pca, y_train)\n",
    "\n",
    "y_pred = classifier.predict(test_pca)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "y_test = 1*y_test\n",
    "z =  accuracy_score(y_test, y_pred)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Visualize all the original dimensions\n",
    "# import plotly.express as px\n",
    "\n",
    "# pca = PCA(n_components=top_k_component)\n",
    "# components = pca.fit_transform(train_X)\n",
    "\n",
    "# total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "# labels = {str(i): f\"PC {i+1}\" for i in range(top_k_component)}\n",
    "# labels['color'] = 'flowInd'\n",
    "\n",
    "# fig = px.scatter_matrix(\n",
    "#     components,\n",
    "#     color=mixed_flows['is_attack'],\n",
    "#     dimensions=range(top_k_component),\n",
    "#     labels=labels,\n",
    "#     title=f'Total Explained Variance: {total_var:.2f}%',\n",
    "# )\n",
    "# fig.update_traces(diagonal_visible=False)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "pca = PCA()\n",
    "pca.fit(train_X)\n",
    "\n",
    "exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(exp_var_cumul)\n",
    "\n",
    "px.area(\n",
    "    x=range(1, exp_var_cumul.shape[0]+1),\n",
    "    y=exp_var_cumul,\n",
    "    labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
